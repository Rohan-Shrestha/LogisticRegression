# -*- coding: utf-8 -*-
"""TitanicDatasetClassification.ipynb

Automatically generated by Colaboratory.

## 1. Importing necessary libraries
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#libraries for Data Analysis
import numpy as np
import pandas as pd

#visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

#ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""## Reading and Exploring the data"""

train = pd.read_csv('/content/drive/MyDrive/train.csv')

"""## Data Analysis

First, we should look at the features of the dataset. The columns are the features of the dataset.
"""

print(train.columns)

"""Each row indicates an observation"""

train.head()

train.sample(5)

"""What is the difference between .head() and .sample() function ?

The .head() function prints the rows from the beginning of the dataset. The default number of rows is 5. We can also define the number of rows we would want to be displayed by passing a number as parameter within the small brackets.

The .sample() funciton prints the rows of the dataset in a random order. The default number of rows is 1. The numbers of rows to be displayed can be defined by giving a number as parameter like in the head function.

### Statistical Analysis
"""

train.describe(include = "all")

#the "all" parameter gives the deatail of every column of the dataset. Including the NaN values.

""" There are a total of 714 values for Age feature in the training set. What is the percentage of missing values in Age column?

"""

percentage = train["Age"].isna().sum()/(train["Age"].isna().sum() + train["Age"].count())*100
# percentage = train["Age"].isna().sum()/len(train["Age"])*100
print("The percentage of missing values of Age column is {}".format(percentage))

"""What is the percentage of missing values in the Cabin feature?"""

percent = train["Cabin"].isna().sum() / len(train["Cabin"]) * 100
# percent = train["Cabin"].isna().sum()/(train["Cabin"].isna().sum() + train["Cabin"].count())*100
print("The percentage of missing values of Cabin column is {}".format(percent))

"""Checking for any other columns which have a lot of NaN values."""

print(pd.isnull(train).sum())

"""# Data Visualization

### Sex Feature
"""

sns.barplot(x = "Sex", y = "Survived", data = train, palette = "Blues")
plt.show()

"""<b> Analysis: </b> Females have a much higher chance of survival than males.

### Pclass Feature
"""

sns.barplot(x = "Pclass", y = "Survived", data = train, palette = "Reds")
plt.show()

"""<b> Analysis: </b> The highest chance of survival are of the people with Pclass 1 whereas the lowest chance of survival are of the people with Plass 3. <br> The hypothesis is true.

### SibSp Feature
"""

sns.barplot(x = "SibSp", y ="Survived", data = train, palette = "Greens")
plt.show()

"""<b> Analysis: </b> The higest chances of survival are of the people who have only 1 spouse or sibling. The more the number of spouse or siblings, the lesser the chance of survival. 
However, people having no spouse or sibling have higher chance of survival than the people with 3 spouse/ sibling but lower than the people with 2 spouse/ sibling.  
<br> The hypothesis is partially true.

### Parch Feature
"""

sns.barplot(x = "Parch", y = "Survived", data = train, palette = "Oranges")
plt.show()

"""<b> Analysis: </b> The people with Parch value 3 have highest change of survival. 
The passenger travelling alone have just a little higher chance of survival than people with Parch value 5. 
<br>
The hypothesis is partially true.

### Age Feature
"""

#sorting the ages into logical categories
train["Age"] = train["Age"].fillna(-0.5)
#filling the missing values with -0.5 and defining them unknown.

bins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]
labels = ["Unknown", "Baby", "Child", "Teenager", "Student", "Young Adult", "Adult", "Senior"]
train["AgeGroup"] = pd.cut(train["Age"], bins, labels = labels) 
# adding a feature named "AgeGroup"

plt.figure(figsize= (12, 7))
sns.barplot(x = "AgeGroup", y = "Survived", data = train)
plt.show()

"""<b> Analysis: </b> The babies are most likely to survive. 
<br>
The hypothesis is true.

# Part II

## Cleaning Data

### Cabin feature
"""

train = train.drop(["Cabin"], axis = 1)

"""### Train feature"""

train = train.drop(["Ticket"], axis = 1)

"""### Embarked feature"""

print("Number of people embarking in Southampton (S): ")
southampton = train[train["Embarked"] == "S"].shape[0]
print(southampton)

print("Number of people emabarking in Cherbourg (C): ")
cherbourg = train[train["Embarked"] == "C"].shape[0]
print(cherbourg)

print("Number of people embarking in Queenstown (Q): ")
queenstown = train[train["Embarked"] == "Q"].shape[0]
print(queenstown)

#replacing the missing values in the Embarked feature with S
train = train.fillna({"Embarked" : "S"})

"""### Age feature"""

# creating a list to loop through all the values of the dataset
combine = [train]

#extracting a title for each Name in the dataset
for dataset in combine:
    dataset["Title"] = dataset.Name.str.extract(' ([A-Za-z]+)\.', expand = False)
    
pd.crosstab(train['Title'], train['Sex'])

#replacing the titles with more common terms

for dataset in combine:
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')
    
    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')
    dataset['Title'] = dataset['Title'].replace('Mlle', "Miss")
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mms', 'Mrs')
    
train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()

"""Did all the Royal titled passengers survive?

Yes. All the passengers with "Royal" title survived.
"""

#defining numerical values for the categorical data

title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Royal": 5, "Rare": 6}
for dataset in combine:
    dataset['Title'] = dataset['Title'].map(title_mapping)
    dataset['Title'] = dataset['Title'].fillna(0)
    
train.head()

mr_age = train[train['Title'] == 1]["AgeGroup"].mode() #Young Adult
miss_age = train[train['Title'] == 2]["AgeGroup"].mode() #Student
mrs_age = train[train['Title'] == 3]["AgeGroup"].mode() #Adult
master_age = train[train['Title'] == 4]["AgeGroup"].mode() #Baby
royal_age = train[train['Title'] == 5]["AgeGroup"].mode() #Adult
rare_age = train[train['Title'] == 6]["AgeGroup"].mode() #Adult

age_title_mapping = {1: "Young Adult", 2: "Student", 3: "Adult", 4: "Baby", 5: "Adult", 6: "Adult"}

train = train.fillna({"Age": train["Title"].map(age_title_mapping)})

for x in range(len(train["AgeGroup"])):
    if train["AgeGroup"][x] == "Unknown":
        train["AgeGroup"][x] = age_title_mapping[train["Title"][x]]

#mapping each Age value to a numerical value
age_mapping = {"Baby" : 1, "Child": 2, "Teenager": 3, "Student": 4, "Young Adult": 5, "Adult": 6, "Senior": 7}
train["AgeGroup"] = train["AgeGroup"].map(age_mapping)

#dropping the Age feature for now, might change
train = train.drop(["Age"], axis = 1)

train.sample(10)

"""### Name feature

Previouly, we have used the name feature to set the null values in the age group but it won't contribute us while training the data for classification. Therefore, we will have to drop this column.
"""

train = train.drop(["Name"], axis = 1)

"""### Sex feature"""

sex_mapping = {"male": 0, "female": 1}
train["Sex"] = train["Sex"].map(sex_mapping)

train.head()

"""### Embarked feature"""

embarked_mapping = {"S": 1, "C": 2, "Q": 3}
train["Embarked"] = train["Embarked"].map(embarked_mapping)

"""### Fare feature"""

train = train.drop(["Fare"], axis = 1)

train.head()

"""What differences you are observing here from the initial dataset?

The initial dataset had a lot of object datatypes and some unnecessary columns that didn't make differece to the output. 
<br> Now, the dataset is clean with all numeric values and ready for training.

Write some other ways to change categoriacal variables into numerical ones? (Hint: Check scikit-learn preprocessing and encoding documentations)

Ordinal encoder and One Hot Encoder

## Choosing the Best Model

### Splitting the Training Data
"""

from sklearn.model_selection import train_test_split

predictors = train.drop(['Survived', 'PassengerId'], axis = 1)
target = train["Survived"]
x_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.20, random_state = 0)

"""### Testing Different Models

### Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logreg = LogisticRegression()
logreg.fit(x_train, y_train)
y_pred = logreg.predict(x_val)
acc_logreg = round(accuracy_score(y_pred, y_val)*100, 2)

print(acc_logreg)

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier 

decisiontree = DecisionTreeClassifier() 
decisiontree.fit(x_train, y_train) 
y_pred = decisiontree.predict(x_val) 
acc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2) 

print(acc_decisiontree)

"""### Support Vector Machines"""

from sklearn.svm import SVC

svc = SVC()
svc.fit(x_train, y_train)
y_pred = svc.predict(x_val)
acc_svc = round(accuracy_score(y_pred, y_val)*100, 2)
print(acc_svc)

"""### Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier 

randomforest = RandomForestClassifier() 
randomforest.fit(x_train, y_train) 
y_pred = randomforest.predict(x_val) 
acc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)

print(acc_randomforest)

"""### KNN or K-Nearest Neighbours"""

from sklearn.neighbors import KNeighborsClassifier 

knn = KNeighborsClassifier() 
knn.fit(x_train, y_train) 
y_pred = knn.predict(x_val) 
acc_knn = round(accuracy_score(y_pred, y_val) * 100, 2) 

print(acc_knn)

models = pd.DataFrame({ 
    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Decision Tree'], 
    'Score': [acc_svc, acc_knn, acc_logreg, acc_randomforest, acc_decisiontree]}) 
models.sort_values(by='Score', ascending=False)

"""Compare all the above models and draw a conclusion which model should we use and why!

We should use KNN(K-Nearest Neighbours) model because the accuracy score of this model is highest.
"""