# -*- coding: utf-8 -*-
"""classification_task.ipynb

Automatically generated by Colaboratory.

Name: Rohan Shrestha
"""

from google.colab import drive
drive.mount('/content/drive')

"""##**Multi-Class Logistic Regression**

Multi-Class Logistic Regression also known as Multinomial Logistic Regression is supervised learning algorithm which is used to classify a non-nominal(categorcal) data. From the name itself, we can know that this algorithm is used to work on the prediction of more than 2 classes in a column of a dataset.

###<b>Importing Libraries Required To Make Classification Model</b>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# from sklearn.metrics import r2_score, mean_squared_error
stars = pd.read_csv('/content/drive/MyDrive/stars.csv')

"""We will be working on a star dataset 7 different variable set(columns).
Among them, Our Target(dependent) Variable will be Star Type and Feature (explanatory) Variables will be Temperature, Luminosity, Radius and Absolute Magnitude.

The star type is categorized into 6 types as given below:
1. Brown Dwarf -> Star Type = 0
2. Red Dwarf -> Star Type = 1
3. White Dwarf-> Star Type = 2
4. Main Sequence -> Star Type = 3
5. Supergiant -> Star Type = 4
6. Hypergiant -> Star Type = 5
"""

stars.head(10)

# dropping two of the categorical columns named star color and spectral class
# the first four columns will be used to make our Multi-Class Logistic Regression Model
stars.drop(['Star color', 'Spectral Class'], inplace = True, axis = 1)

stars.head()

stars.shape

stars.isnull()

stars.info()

stars.describe()

# ckecking for class imbalance
stars['Star type'].value_counts()

"""From the the above count values, we can say that our target variable is evenly divided among all the star types.

###**#Scatter Plot of Stars based on their Temperature and Absolute Magnitude on left and Temperature and Luminosity on the right**

A scatter plot showing a graph which is followed by stars in the astronomical(galactic) space is plotted below (left) using the "6 star csv.csv" dataset. The diagram is commonly known as Hertzsprung-Russell Diagram or shortly HR-Diagram. In the scatter plot below, the stars differentiated by their types are plotted on the basis of their temperature and Absolute Magnitude
"""

fig,ax=plt.subplots(nrows=1,ncols=2,figsize=(13,9))
sns.scatterplot(x='Temperature (K)',y='Absolute magnitude(Mv)',data=stars,hue='Star type',ax=ax[0],palette='dark')
sns.scatterplot(x='Temperature (K)',y='Luminosity(L/Lo)',data=stars,hue='Star type',ax=ax[1],palette='dark')
plt.tight_layout()
plt.show()

import seaborn as sns
sns.scatterplot(x='Temperature (K)',y='Absolute magnitude(Mv)',hue='Star type',data=stars)

X = stars.iloc[ : , :4] # initializing the columns from 1 to 4 as the feature variables (X)
Y = stars.iloc[ : , 4] # initializing the last column after dropping columns as the dependent variable (Y)

X

Y

for i in range(1, len(X.columns)): #for loop to iterate through all the columns in the dataset
 X.iloc[:, i] = (X.iloc[:, i]-np.min(X.iloc[:, i]))/(np.max(X.iloc[:, i])-np.min(X.iloc[:, i]))
 #applying the normalization formula

X.head() #printing the dataset after Normalization

#putting 70% of the dataset into train data and the other 30% into test data with random state 41
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,train_size=0.7, random_state=41)

classifier = LogisticRegression(C=5.0,max_iter=10000,multi_class="multinomial",solver='newton-cg', penalty='l2')

classifier.fit(X_train, Y_train)

Y_pred = classifier.predict(X_test)

cm = confusion_matrix(Y_test, Y_pred)
cm

"""##<b>Confusion Matrix</b>
-By observing confusion matrix we can predict how good is a model.
-
"""

plt.figure(figsize = (10,7))
sns.heatmap(cm, annot=True)
plt.xlabel('Predicted')
plt.ylabel('Truth')

print (pd.DataFrame(cm, columns=['Brown Dwarf=0','Red Dwarf=1','White Dwarf=2','Main Sequence=3','Supergiant=4','Hypergiant=5']))

print(classification_report(Y_test, Y_pred))

# plt.style.use('ggplot')
# plt.figure(figsize = (20,15))
# plt.subplot(2,2,1)
# sns.boxplot(x = 'Temperature (K)', y = 'Spectral Class', data = stars)
# plt.subplot(2,2,2)
# sns.boxplot(x = 'Temperature (K)', y = 'Star color', data = stars)

# calculating the accuracy of our classification Model
accuracy = classifier.score(X_test, Y_test)
print("Accuracy = ", accuracy)

"""##**AUC ROC CURVE**
AUC ROC Curve stands for "Area Under the Curve" of "Receiver Charecterstic Operator"

The main purposes of AUC ROC Curve is to help analyze the performance of our machine learning model (Here, Multi-Class Logistic Regression).

AUC ROC curves are generally used for classification of binary values which follows one vs one (OVO) technique, but we can use it for multi-class also using the One Vs Rest technique (OVR)
OVR Technique computes the AUC of each class against the rest classes. In our dataset we have 6 different classes. Therefore, the ROC computation of class 0 is done by classifying class 0 against rest of the class i.e. 1,2,3,4,5. ROC computation of class 1 is done by classifying class 1 against rest of the class i.e. 0,2,3,4,5 and so on.
"""

# importing libraries to calculate AUC ROC score and plot roc_curve
from sklearn.datasets import make_classification
from sklearn.multiclass import OneVsRestClassifier  # classifier required for multi-class auc roc curves
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

# generating 2 class dataset
# default values: sample = 100, classes = 2, features = 20, informative = 2, redundant = 2, repetitive = 0,
# classes = 2, clusters_per_class = 2, weights, flip_yfloat, default=0.01, class_sepfloat, default=1.0, hypercubebool, default=True,
# shift = 0.0, scale = 1.0, shuffle = True, random scale = None
X, Y = make_classification(n_samples=1000, n_classes=6, n_features=20, n_informative=8, random_state=42)

# split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)

# fitting the model
clf = OneVsRestClassifier(LogisticRegression())
clf.fit(X_train, y_train)
pred = clf.predict(X_test)
# probability prediction
pred_prob = clf.predict_proba(X_test)

# roc curve for classes
fpr = {}
tpr = {}
thresh ={}

n_class = 6

for i in range(n_class):    
    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)
    
# plotting
plt.figure(figsize = (15,12))
# plotting the ROC curve of class 0 = Brown Dwarf
plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')
# plotting the ROC curve of class 1 = Red Dwarf
plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')
# plotting the ROC curve of class 2 = White Giant
plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')
# plotting the ROC curve of class 3 = Main Sequence
plt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')
# plotting the ROC curve of class 4 = Super Sequence
plt.plot(fpr[4], tpr[4], linestyle='--',color='purple', label='Class 4 vs Rest')
# plotting the ROC curve of class 5 = Hyper Giant
plt.plot(fpr[5], tpr[5], linestyle='--',color='black', label='Class 5 vs Rest')

plt.title('Multiclass ROC curve')  # labelling the Graph Title
plt.xlabel('False Positive Rate')  # labelling at X axis
plt.ylabel('True Positive rate')  # labelling at Y axis
# specifying the location of the legend with the default value
plt.legend(loc='best')
# dots per inch set to 300
plt.savefig('Multiclass ROC',dpi=300);